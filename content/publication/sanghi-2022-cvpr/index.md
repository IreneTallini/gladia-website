---
# Documentation: https://wowchemy.com/docs/managing-content/

title: 'CLIP-Forge: Towards Zero-Shot Text-To-Shape Generation'
subtitle: ''
summary: ''
authors:
- Aditya Sanghi
- Hang Chu
- Joseph G. Lambourne
- Ye Wang
- Chin-Yi Cheng
- fumero
- Kamal Rahimi Malekshan
tags: []
categories: []
date: '2022-06-01'
lastmod: 2023-02-08T15:02:08+01:00
featured: false
draft: false
publication_short: "CVPR 2022"

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2023-02-08T14:02:07.488972Z'
publication_types:
- '1'
abstract: 'Generating shapes using natural language can enable new ways of imagining
  and creating the things around us. While significant recent progress has been made
  in text-to-image generation, text-to-shape generation remains a challenging problem
  due to the unavailability of paired text and shape data at a large scale. We present
  a simple yet effective method for zero-shot text-to-shape generation that circumvents
  such data scarcity. Our proposed method, named CLIP-Forge, is based on a two-stage
  training process, which only depends on an unlabelled shape dataset and a pre-trained
  image-text network such as CLIP. Our method has the benefits of avoiding expensive
  inference time optimization, as well as the ability to generate multiple shapes
  for a given text. We not only demonstrate promising zero-shot generalization of
  the CLIP-Forge model qualitatively and quantitatively, but also provide extensive
  comparative evaluations to better understand its behavior. '
publication: '*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)*'
---
